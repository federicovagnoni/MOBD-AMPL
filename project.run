reset;
model project.mod;
data project.dat;

param lb;
param ub;


# ----------------------------------
# Lettura e scaling del training set
# ----------------------------------
for {p in 1..ptr}{
	for {k in 1..n-1}{
		read xtr[k,p] < training_por.txt;
	}
	read ytr[p] < training_por.txt;
}

# Scale of age
let lb := min{p in 1..ptr} xtr[1,p];
let ub := max{p in 1..ptr} xtr[1,p];

let{p in 1..ptr} xtr[1,p] := ((2*(xtr[1,p] - lb)) / (ub - lb)) - 1;

# Scale of Medu, Fedu, traveltime, studytime, failures
for {i in 3..7} {
	let lb := min{p in 1..ptr} xtr[i,p];
	let ub := max{p in 1..ptr} xtr[i,p];

	let{p in 1..ptr} xtr[i,p] := ((2*(xtr[i,p] - lb)) / (ub - lb)) - 1;
	
}

# traveltime and failures are valued opposite manner
let {p in 1..ptr} xtr[5,p] := -xtr[5,p];
let {p in 1..ptr} xtr[7,p] := -xtr[7,p];



for {i in 16..24} {
	let lb := min{p in 1..ptr} xtr[i,p];
	let ub := max{p in 1..ptr} xtr[i,p];

	let{p in 1..ptr} xtr[i,p] := ((2*(xtr[i,p] - lb)) / (ub - lb)) - 1;
}

# freetime, goout, Dalc, Walc are valued opposite manner
for {i in 17..20} {
	let {p in 1..ptr} xtr[i,p] := -xtr[i,p];
}

# absences are valued on opposite manner
let {p in 1..ptr} xtr[22,p] := -xtr[22,p];


# Scale G3
let lb := min{p in 1..ptr} ytr[p];
let ub := max{p in 1..ptr} ytr[p];

let{p in 1..ptr} ytr[p] := ((2*(ytr[p] - lb)) / (ub - lb)) - 1;


# ------------------------------------
# Lettura e scaling del validation set
# ------------------------------------
for {p in 1..pval}{
	for {k in 1..n-1}{
		read xval[k,p] < validation_por.txt;
	}
	read yval[p] < validation_por.txt;
}


# Scale of age
let lb := min{p in 1..pval} xval[1,p];
let ub := max{p in 1..pval} xval[1,p];

let{p in 1..pval} xval[1,p] := ((2*(xval[1,p] - lb)) / (ub - lb)) - 1;


# Scale of Medu, Fedu, traveltime, studytime, failures
for {i in 3..7} {
	let lb := min{p in 1..pval} xval[i,p];
	let ub := max{p in 1..pval} xval[i,p];

	let{p in 1..pval} xval[i,p] := ((2*(xval[i,p] - lb)) / (ub - lb)) - 1;
	
}

# traveltime and failures are valued on the opposite
let {p in 1..pval} xval[5,p] := -xval[5,p];
let {p in 1..pval} xval[7,p] := -xval[7,p];


for {i in 16..24} {
	let lb := min{p in 1..pval} xval[i,p];
	let ub := max{p in 1..pval} xval[i,p];

	let{p in 1..pval} xval[i,p] := ((2*(xval[i,p] - lb)) / (ub - lb)) - 1;
}

# freetime, goout, Dalc, Walc are valued on the opposite
for {i in 17..20} {
	let {p in 1..pval} xval[i,p] := -xval[i,p];
}

# absences are valued on the opposite
let {p in 1..pval} xval[22,p] := -xval[22,p];


# Scale G3
let lb := min{p in 1..pval} yval[p];
let ub := max{p in 1..pval} yval[p];

let{p in 1..pval} yval[p] := ((2*(yval[p] - lb)) / (ub - lb)) - 1;


# Start training

	
#let{i in 1..nl} w[i] := Uniform(-100,100);	
#let{i in 1..n-1, j in 1..nl} c[i,j] := Uniform(-100,100);
let{i in 1..nl} v[i] := Uniform(-10,10);
let{z in 1..n, j in 1..nl} win[z,j] := Uniform(-10,10);	

let err_tr := 1.e30;
let err_val := 1.e30;

repeat while (stop_tr == 0 ){
	
	let loc_err_tr := 1.e30;
	let loc_err_val := 1.e30;
	
	#multistart for nl neurons in the hidden layer
	for {k in 1..100} {
				
		let {i in 1..n, j in 1..nl} win[i, j] := Uniform(-10,10);
		let {i in 1..nl} v[i]:= Uniform(-10,10);
		option solver knitro;
		option knitro_options "opttol = 1.0e-5";
		option solver_msg 0;
		solve;
		
		#saving best error on the validation set for nl neurons
		if (error_val <= loc_err_val) then { 
			printf "error on validation improved: old %f new %f\n", loc_err_val, error_val;
			let loc_err_val := error_val;
			let loc_err_tr := error_tr;
		}
		let best_k_loc := k;
	} 

	#check for a global improvement on the validation set
	if (loc_err_val < err_val) then {
	#updating the best error and increasing the number of neurons
		printf "the best error on the validation set is %f with %d neurons \n",
			loc_err_val,nl;
		let best_nl := nl; 
		let best_k := best_k_loc;
		let nl := nl+1;
		let err_val := loc_err_val;
		let err_tr := loc_err_tr;
	}
	else 
	#stop
		let stop_tr := 1;
}#while

display best_nl;
display err_val;


