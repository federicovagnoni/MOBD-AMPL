reset;
model project.mod;
data project.dat;

param lb;
param ub;
param lb_f;
param ub_f;
param mean;
param std;


# Read and scale
include readandscale.run;

for {i in 22..24} {
	let{p in 1..ptr} xtr2[i-21,p] := xtr[i,p];
}

for {i in 22..24} {
	let{p in 1..pval} xval2[i-21,p] := xval[i,p];
}

for {i in 22..24} {
	let{p in 1..ptest} xtest2[i-21,p] := xtest[i,p];
}


# Start training
let err_tr := 1.e30;
let err_val := 1.e30;
let err_tst := 1.e30;


repeat while (stop_tr == 0 ){
	
	let loc_err_tr := 1.e30;
	let loc_err_val := 1.e30;
	let loc_err_tst:=1.e30;
	
	#multistart for nl neurons in the hidden layer
	for {k in 1..100} {
				
		#let{i in 1..nl} w[i] := Uniform(-100,100);	
		#let{i in 1..n-1, j in 1..nl} c[i,j] := Uniform(-100,100);
		let{i in 1..nl} v[i] := Uniform(-10,10);
		let{z in 1..n, j in 1..nl} win[z,j] := Uniform(-10,10);		
		
		option solver knitro;
		option knitro_options "opttol = 1.0e-5";
		option solver_msg 0;
		solve;
		
		#saving best error on the validation set for nl neurons
		if (error_val <= loc_err_val) then { 
			printf "error on validation improved: old %f new %f\n", loc_err_val, error_val;
			let loc_err_val := error_val;
			let loc_err_tr := error_tr;
			let loc_err_tst:= error_test;

		}
		let best_k_loc := k;
	} 

	#check for a global improvement on the validation set
	if (loc_err_val < err_val) then {
	#updating the best error and increasing the number of neurons
		printf "the best error on the validation set is %f with %d neurons \n",
			loc_err_val,nl;
		let best_nl := nl; 
		let best_k := best_k_loc;
		let nl := nl+1;
		let err_val := loc_err_val;
		let err_tr := loc_err_tr;
		let err_tst := loc_err_tst;

	}
	else 
	#stop
		let stop_tr := 1;
}#while

display best_nl;
display err_tr;
display err_val;
printf "final error on the test set is %f\n", err_tst;



